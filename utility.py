import torch
import numpy as np
import os
import scipy.io
import matplotlib.pylab as plt
import json

def set_device(device):
    '''
    set device to cuda if avaliable otherwise device will be cpu
    '''
    if (device == 'cuda') & torch.cuda.is_available():
        torch.set_default_tensor_type(torch.cuda.FloatTensor)
    else:
        device = 'cpu'
    return device

@torch.no_grad()
def get_response(x, net):
    '''
    get impulse and magnitude resoponse generated by the learned parameters of FDN()
    Args    net (nn.Module): trained FDN() network
            num (int): number of frequnecy samples to evaluate the impulse response on
    Output  h (torch.tensor): FDN impulse response
            H (torch.tensor): FDN magnitude response
    '''
    with torch.no_grad():
        H = net(x)
        H = torch.sum(H, dim=-1)
        h = torch.fft.irfft(H)
    return H, h

def get_frequency_samples(num):
    '''
    get frequency samples (in radians) sampled at linearly spaced points along the unit circle
    Args    num (int): number of frequency samples
    Output  frequency samples in radians between [0, pi]
    '''
    # angle = torch.arange(0, 1+(1/num)/2, 1/num)
    # abs = torch.ones(num+1)
    angle = torch.linspace(0, 1, num)
    abs = torch.ones(num)
    return torch.polar(abs, angle * np.pi)    

def weights_init_normal(m):
    '''
    Takes in a module and initializes all linear layers with weight
        values taken from a normal distribution.
    '''

    classname = m.__class__.__name__
    # for every Linear layer in a model
    if classname.find('Linear') != -1:
        y = m.in_features * m.in_features
        # m.weight.data shoud be taken from a normal distribution
        m.weight.data.normal_(0.0,1/np.sqrt(y))
        # m.bias.data should be 0
        m.bias.data.fill_(0)

def save_parameters(net, dir_path, filename):
    '''
    save parameters of FDN() net to .json file 
    Args    net (nn.Module): trained FDN() network
            dir_path (string): path to output firectory
            filename (string): name of the file 
    Output  param (dictionary of tensors): FDN() net parameters
            param_np (dictionary of numpy arrays): FDN() net parameters
    '''
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)   
    
    param = fdn2dir(net)
    param_np = {}
    for name, value in param.items():
        try:
            param_np[name] = value.squeeze().cpu().numpy()
        except AttributeError:
            param_np[name] = value
    
    # save parameters in numpy format 
    # wat soll die matlab scheisse hier, lass das mal ändern, parameter können als json gespeichert werden
    #scipy.io.savemat(os.path.join(dir_path, filename),
    #                param_np)
    
    with open(filename, "w") as f:
        json.dump(param_np, f)

    return param, param_np

def save_filters(net, filter_designer, dir_path, filename):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path) 
    param_np = {}
    param_np['G_SOS'] = net.G_SOS
    # param_np['G'] = net.G.squeeze().cpu().numpy()  
    param_np['TC_SOS'] = net.TC_SOS
    # param_np['TC'] = net.TC.squeeze().cpu().numpy()  
    param_np['T'] = filter_designer.T
    param_np['A'] = filter_designer.A
    param_np['N'] = filter_designer.N
    param_np['f_bands'] = filter_designer.f_bands
    # save parameters in numpy format 
    scipy.io.savemat(os.path.join(dir_path, filename),
                    param_np)

def fdn2dir(net):
    '''
    save learnable parameters to a dictionary  
    Args    net (nn.Module): trained FDN() network
    Output  d (dictionary of tensors): FDN() net parameters 
    '''
    d = {} # enpty dictionary 
    for name, param in net.named_parameters():
        if param.requires_grad:
            d[name] = param.data 
    #d['gain_per_sample'] = net.gain_per_sample
    d['N'] = net.N
    return d


def get_str_results(epoch=None, train_loss=None, valid_loss=None, time=None, lossF = None, lossT = None):
    '''construct the string that has to be print at the end of the epoch'''
    to_print=''

    if epoch is not None:
        to_print += 'epoch: {:3d} '.format(epoch)
    
    if train_loss is not None:
        to_print += '- train_loss: {:6.4f} '.format(train_loss[-1])
                        
    if valid_loss is not None:
        to_print += '- valid_loss: {:6.4f} '.format(valid_loss[-1])

    if time is not None:
        to_print += '- time: {:6.4f} s'.format(time)

    if lossF is not None:
        to_print += '- lossF: {:6.4f}'.format(lossF) 

    if lossT is not None:
        to_print += '- lossT: {:6.4f}'.format(lossT) 

    return to_print

def save_loss(train_loss, valid_loss, output_dir, save_plot=True, filename=''):
    '''
    save training and validation loss values in keinem scheiss matlab format
    '''
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)  
        
    losses = {}
    losses['train'] = train_loss
    losses['valid'] = valid_loss
    n_epochs = len(train_loss)
    
    np.save(os.path.join(output_dir, "train_loss.npy"), train_loss)
    np.save(os.path.join(output_dir, "val_loss.npy"), valid_loss)
    
    if save_plot:
        plt.plot(range(n_epochs), train_loss, label='training', marker="o")
        plt.plot(range(n_epochs), valid_loss, label='validation', marker="o")
        plt.legend()
        plt.savefig(os.path.join(output_dir,'losses'+filename+'.pdf'))
        plt.close()

def to_complex(X):
    return torch.complex(X, torch.zeros_like(X))